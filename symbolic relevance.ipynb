{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['class', 'document', 'feature']\n",
    "pubmedClass = []\n",
    "pubmedDoc = [] \n",
    "pubmedFeature = []\n",
    "\n",
    "# import pubmed\n",
    "docList = glob.glob(os.path.join(os.getcwd(), \"Datasets/pubmed/\", \"*.txt\"))\n",
    "\n",
    "for docPath in docList:\n",
    "    docName = os.path.basename(docPath).split('.')[0]\n",
    "    pubmedDoc.append(docName)\n",
    "    pubmedClass.append(docName[:3])\n",
    "    with open(docPath) as doc:\n",
    "        pubmedFeature.append(doc.read().replace('\\n', ' '))\n",
    "\n",
    "# print(pubmed)\n",
    "\n",
    "# export csv\n",
    "def exportCsv(classes, docs, features, fileName):\n",
    "    with open(f'{fileName}.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        for i in range(0, len(features)):\n",
    "            writer.writerow([classes[i], docs[i], features[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export pubmed raw\n",
    "exportCsv(pubmedClass, pubmedDoc, pubmedFeature, 'pubmed_raw')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>document</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALZ</td>\n",
       "      <td>ALZ1</td>\n",
       "      <td>Reduced amounts of immunoreactive somatostatin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALZ</td>\n",
       "      <td>ALZ10</td>\n",
       "      <td>Diagnostic criteria for primary neuronal degen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALZ</td>\n",
       "      <td>ALZ100</td>\n",
       "      <td>14Cacetylcholine synthesis and 14Ccarbon dioxi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALZ</td>\n",
       "      <td>ALZ1000</td>\n",
       "      <td>The pattern of reading deterioration in dement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALZ</td>\n",
       "      <td>ALZ101</td>\n",
       "      <td>Cerebral blood flow and metabolic rate of oxyg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class document                                            feature\n",
       "0   ALZ     ALZ1  Reduced amounts of immunoreactive somatostatin...\n",
       "1   ALZ    ALZ10  Diagnostic criteria for primary neuronal degen...\n",
       "2   ALZ   ALZ100  14Cacetylcholine synthesis and 14Ccarbon dioxi...\n",
       "3   ALZ  ALZ1000  The pattern of reading deterioration in dement...\n",
       "4   ALZ   ALZ101  Cerebral blood flow and metabolic rate of oxyg..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv\n",
    "dataRaw = pd.read_csv('pubmed_raw.csv')\n",
    "dataRaw.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataRaw.loc[:, 'feature']\n",
    "\n",
    "# punctuation removal\n",
    "punctuation = []\n",
    "for feature in features:\n",
    "    translate = feature.translate(str.maketrans('', '', string.punctuation))\n",
    "    punctuation.append(translate)\n",
    "\n",
    "# print(punctuation)\n",
    "\n",
    "# case folding\n",
    "casefolding = []\n",
    "for feature in punctuation:\n",
    "    lower = feature.lower()\n",
    "    casefolding.append(lower)\n",
    "\n",
    "# print(casefolding)\n",
    "\n",
    "# tokenization\n",
    "tokenization = []\n",
    "for feature in casefolding:\n",
    "    token = word_tokenize(feature)\n",
    "    tokenization.append(token)\n",
    "\n",
    "# print(tokenization)\n",
    "\n",
    "# stopwords removal\n",
    "preprocessedFeature = []\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "for token in tokenization:\n",
    "    cleanedFeature = []\n",
    "    for feature in token:\n",
    "        if feature not in stopWords:\n",
    "            cleanedFeature.append(feature)\n",
    "    preprocessedFeature.append(cleanedFeature)\n",
    "\n",
    "# print(preprocessedFeature)\n",
    "\n",
    "# export pubmed clean\n",
    "exportCsv(pubmedClass, pubmedDoc, preprocessedFeature, 'pubmed_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>document</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALZ</td>\n",
       "      <td>ALZ1</td>\n",
       "      <td>['reduced', 'amounts', 'immunoreactive', 'soma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALZ</td>\n",
       "      <td>ALZ10</td>\n",
       "      <td>['diagnostic', 'criteria', 'primary', 'neurona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALZ</td>\n",
       "      <td>ALZ100</td>\n",
       "      <td>['14cacetylcholine', 'synthesis', '14ccarbon',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALZ</td>\n",
       "      <td>ALZ1000</td>\n",
       "      <td>['pattern', 'reading', 'deterioration', 'demen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALZ</td>\n",
       "      <td>ALZ101</td>\n",
       "      <td>['cerebral', 'blood', 'flow', 'metabolic', 'ra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class document                                            feature\n",
       "0   ALZ     ALZ1  ['reduced', 'amounts', 'immunoreactive', 'soma...\n",
       "1   ALZ    ALZ10  ['diagnostic', 'criteria', 'primary', 'neurona...\n",
       "2   ALZ   ALZ100  ['14cacetylcholine', 'synthesis', '14ccarbon',...\n",
       "3   ALZ  ALZ1000  ['pattern', 'reading', 'deterioration', 'demen...\n",
       "4   ALZ   ALZ101  ['cerebral', 'blood', 'flow', 'metabolic', 'ra..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv\n",
    "dataClean = pd.read_csv('pubmed_clean.csv')\n",
    "dataClean.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Forming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ZephZ/nltk_data'\n    - 'd:\\\\Downloads\\\\Anaconda\\\\nltk_data'\n    - 'd:\\\\Downloads\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'd:\\\\Downloads\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ZephZ\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26324\\3134404718.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# POS tag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtagged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'you'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'where'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'animal'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Downloads\\Anaconda\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \"\"\"\n\u001b[1;32m--> 165\u001b[1;33m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Downloads\\Anaconda\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Downloads\\Anaconda\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             AP_MODEL_LOC = \"file:\" + str(\n\u001b[1;32m--> 167\u001b[1;33m                 \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"taggers/averaged_perceptron_tagger/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mPICKLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m             )\n\u001b[0;32m    169\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Downloads\\Anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ZephZ/nltk_data'\n    - 'd:\\\\Downloads\\\\Anaconda\\\\nltk_data'\n    - 'd:\\\\Downloads\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'd:\\\\Downloads\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ZephZ\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "features = dataClean.loc[:, 'feature']\n",
    "\n",
    "# POS tag\n",
    "tagged = nltk.pos_tag(features)\n",
    "\n",
    "# BOAW\n",
    "def boaw(features):\n",
    "    vectorizer = CountVectorizer()\n",
    "    boaw = vectorizer.fit_transform(features).todense()\n",
    "    return boaw\n",
    "\n",
    "boaw(tagged)\n",
    "\n",
    "# BON\n",
    "def bon(features):\n",
    "    list = []\n",
    "    for (text, tag) in features:\n",
    "        if tag == 'NOUN':\n",
    "            list.append(text)\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    bon = vectorizer.fit_transform(list).todense()\n",
    "    return bon\n",
    "\n",
    "# BONA\n",
    "def bona(features):\n",
    "    list = []\n",
    "    for (text, tag) in features:\n",
    "        if tag == 'NOUN' or tag == 'ADJ':\n",
    "            list.append(text)\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    bona = vectorizer.fit_transform(list).todense()\n",
    "    return bona"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF\n",
    "def tf(features):\n",
    "    vectorizer = CountVectorizer()\n",
    "    tf = vectorizer.fit_transform(features).todense()\n",
    "    return tf\n",
    "\n",
    "# TF-IDF\n",
    "def tf_idf(features):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfIdf = vectorizer.fit_transform(features)\n",
    "    return tfIdf\n",
    "\n",
    "# TF-IDF-ICF\n",
    "def tf_idf_icf(features):\n",
    "    tfIdf = tf_idf(features)\n",
    "    # tfIdfIcf = tfIdf * icf\n",
    "    # return tfIdfIcf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60e3baaf0d9150201909656e65448b29902506d837bde84e10252942c4c1f9ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
