{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "pubmed = []\n",
    "\n",
    "# import pubmed\n",
    "docList = glob.glob(os.path.join(os.getcwd(), \"Datasets/pubmed/\", \"*.txt\"))\n",
    "\n",
    "for docPath in docList:\n",
    "    # get doc file name\n",
    "    docName = os.path.basename(docPath).split('.')[0]\n",
    "    \n",
    "    with open(docPath) as doc:\n",
    "        # insert [class, docs, feature]\n",
    "        pubmed.append([docName[:3], docName, doc.read().replace('\\n', ' ')])\n",
    "\n",
    "# print(pubmed)\n",
    "\n",
    "# make dataframe\n",
    "dataframe = pd.DataFrame(data=pubmed, columns=['class', 'document', 'feature']) \n",
    "\n",
    "# export pubmed raw\n",
    "dataframe.to_csv('pubmed/raw.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "dataRaw = pd.read_csv('pubmed/raw.csv')\n",
    "# get feature\n",
    "features = dataRaw.loc[:, 'feature']\n",
    "dataRaw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# cleaning\n",
    "def cleaning(features):\n",
    "    result = []\n",
    "    for feature in features:\n",
    "        regex = re.sub(r'[^a-zA-Z\\s]', '', feature)\n",
    "        result.append(regex)\n",
    "    return result\n",
    "\n",
    "# case folding\n",
    "def caseFolding(features):\n",
    "    result = []\n",
    "    for feature in features:\n",
    "        lower = feature.lower()\n",
    "        result.append(lower)\n",
    "    return result\n",
    "\n",
    "# tokenization\n",
    "def tokenization(features):\n",
    "    result = []\n",
    "    for feature in features:\n",
    "        token = word_tokenize(feature)\n",
    "        result.append(token)\n",
    "    return result\n",
    "\n",
    "# stopwords removal\n",
    "def stopWords(features):\n",
    "    result = []\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    for token in features:\n",
    "        cleanedFeature = [feature for feature in token if feature not in stopWords]\n",
    "        result.append(cleanedFeature)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning\n",
    "clean = cleaning(features)\n",
    "# print(clean)\n",
    "\n",
    "case = caseFolding(clean)\n",
    "# print(casefolding)\n",
    "\n",
    "token = tokenization(case)\n",
    "# print(tokenization)\n",
    "\n",
    "preprocessed = stopWords(token)\n",
    "# print(preprocessedFeature)\n",
    "\n",
    "# export pubmed clean\n",
    "for i in range(len(preprocessed)):\n",
    "    dataRaw.loc[i, 'feature'] = ' '.join(preprocessed[i])\n",
    "dataRaw.to_csv('pubmed/clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "dataClean = pd.read_csv('pubmed/clean.csv')\n",
    "# get feature\n",
    "features = dataClean.loc[:, 'feature']\n",
    "dataClean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Forming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# BOAW\n",
    "dataClean.rename(columns={'feature': 'BOAW'}, inplace=True)\n",
    "\n",
    "for i in tqdm(range(len(features))):\n",
    "    # BON\n",
    "    dataClean.loc[i,'BON'] = ' '.join(TextBlob(features[i]).noun_phrases)\n",
    "    # BONA\n",
    "    dataClean.loc[i,'BONA'] = ' '.join([word for (word, tag) in TextBlob(features[i]).tags if tag[:2]=='NN' or tag[:2]=='JJ'])\n",
    "\n",
    "# print(dataClean)\n",
    "\n",
    "dataClean.to_csv('pubmed/formed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "dataFormed = pd.read_csv('pubmed/formed.csv')\n",
    "# get features\n",
    "classes = dataFormed.loc[:, 'class']\n",
    "boaw = dataFormed.loc[:, 'BOAW']\n",
    "bon = dataFormed.loc[:, 'BON']\n",
    "bona = dataFormed.loc[:, 'BONA']\n",
    "dataFormed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF\n",
    "def tf(features):\n",
    "    tfVec = CountVectorizer()\n",
    "    result = tfVec.fit_transform(features)\n",
    "\n",
    "    featureName = tfVec.get_feature_names_out()\n",
    "    featureWeight = result.todense().tolist()\n",
    "    df = pd.DataFrame(featureWeight, columns=featureName)\n",
    "    return df\n",
    "\n",
    "tfx = tf(bona)\n",
    "print(tfx)\n",
    "\n",
    "# TF-IDF\n",
    "def tf_idf(features):\n",
    "    tfIdfVec = TfidfVectorizer()\n",
    "    result = tfIdfVec.fit_transform(features)\n",
    "\n",
    "    featureName = tfIdfVec.get_feature_names_out()\n",
    "    featureWeight = result.todense().tolist()\n",
    "    df = pd.DataFrame(featureWeight, columns=featureName)\n",
    "    return df\n",
    "\n",
    "# idfx = tf_idf(bona)\n",
    "# print(idfx)\n",
    "\n",
    "# TF-IDF-ICF\n",
    "def icf(features, classes):\n",
    "  icf = []\n",
    "  C = []\n",
    "\n",
    "  # count class\n",
    "  for i in classes:\n",
    "    if i not in C:\n",
    "      C.append(i)\n",
    "\n",
    "  # count term\n",
    "#   for feature in features:\n",
    "#     for i in C:\n",
    "#       if \n",
    "    \n",
    "  \n",
    "#   for word, val in idfDict.items():\n",
    "#       idfDict[word] = math.log(N / float(val))\n",
    "  \n",
    "  return icf\n",
    "\n",
    "def tf_idf_icf(features):\n",
    "    tfIdfIcf = []\n",
    "    tfIdf = tf_idf(features)\n",
    "\n",
    "    for word, val in tfIdf.items():\n",
    "        tfIdfIcf[word] = val * icf[word]\n",
    "        break\n",
    "    return tfIdfIcf\n",
    "\n",
    "# icfx = tf_idf_icf(features)\n",
    "# print(icfx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab696253bf1cbd9262a120019f89a6af5e719602af9132e0a92b18d085125844"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
