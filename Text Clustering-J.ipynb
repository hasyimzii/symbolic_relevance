{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PENINGKATAN PERFORMA CLUSTERING PADA LARGE TEXT DATASET MENGGUNAKAN STAMANTIC SPHERICAL K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import json\n",
    "from nltk.corpus import wordnet\n",
    "# import wn, wn.taxonomy\n",
    "from strsimpy import SIFT4\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "Mengolah raw txt dataset dan menggabungkannya menjadi satu file dengan format CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pubmed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pubmed_data_collection():\n",
    "    #looping nama file\n",
    "    symbol = ['ALZ', 'CAN', 'DBT', 'HIV']\n",
    "    filesname = []\n",
    "    docs = []\n",
    "    for i in range(1, 4001):\n",
    "        if 1 <= i <= 1000:\n",
    "            filename = f'{symbol[0]}{i}.txt' \n",
    "            filesname.append(filename)\n",
    "        elif 1001 <= i <= 2000:\n",
    "          filename = f'{symbol[1]}{i}.txt' \n",
    "          filesname.append(filename)\n",
    "        elif 2001 <= i <= 3000:\n",
    "          filename = f'{symbol[2]}{i}.txt' \n",
    "          filesname.append(filename)\n",
    "        elif 3001 <= i <= 4000:\n",
    "          filename = f'{symbol[3]}{i}.txt' \n",
    "          filesname.append(filename)\n",
    "        else:\n",
    "            print('out of range')\n",
    "\n",
    "    try:\n",
    "        #looping tiap file\n",
    "        index = 0\n",
    "        for files in filesname:\n",
    "            file = open(f\"D:/text clustering/PUBMED 4K/{files}\",\"r\")\n",
    "            content = file.read().splitlines() #list berisi tiap baris dalam file\n",
    "            doc = {}\n",
    "            if 0 <= index <= 999:\n",
    "                doc['label'] = 'alzheimer'\n",
    "            elif 1000 <= index <= 1999:\n",
    "                doc['label'] = 'cancer'\n",
    "            elif 2000 <= index <= 2999:\n",
    "                doc['label'] = 'diabetes'\n",
    "            elif 3000 <= index <= 3999:\n",
    "                doc['label'] = 'HIV'\n",
    "            else:\n",
    "                print('index out of range')\n",
    "            #looping tiap baris dalam 1 file, input ke list\n",
    "            lines = []\n",
    "            for line in content:\n",
    "                lines.append(line)\n",
    "            #membuat dict tiap file\n",
    "            for l in lines:\n",
    "                doc['title'] = lines[0]\n",
    "                list_abstract = lines[1:]\n",
    "                doc['abstract'] = list_abstract[0] #mengeluarkan value dari list abstract biar jadi string\n",
    "                doc['merged'] = doc['title'] + ' ' + doc['abstract']\n",
    "            docs.append(doc)\n",
    "            index += 1\n",
    "            \n",
    "        #display data\n",
    "        docs_df = pd.DataFrame.from_dict(docs)\n",
    "        # print(docs_df)\n",
    "\n",
    "        #dataframe to csv\n",
    "        # docs_df.to_csv('D:/text clustering/pubmed.csv', index=False, encoding='utf-8')\n",
    "\n",
    "        #check data\n",
    "        # print(f'Jumlah data null : {docs_df.isnull().sum()}\\n')\n",
    "        # print(f'Total data null : {docs_df.isnull().sum().sum()}\\n')\n",
    "        # print(f'Data Duplikat : {docs_df.duplicated()}\\n')\n",
    "    except IOError as e:\n",
    "        print(IOError, e)\n",
    "\n",
    "pubmed_data_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scopus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scopus_data_collection():\n",
    "    #looping nama file\n",
    "    name = ['CONCRETE-2015-', 'HYPERACTIVITY-2015-', 'INVESTMENT-2015-', 'NEURALNETWORK-2015-', 'PHOTOSYNTHESIS-2015-', 'PROTON-2015-', 'TECTONICPLATES-2015-']\n",
    "    filesname = []\n",
    "    docs = []\n",
    "    for i in range(1, 404): #concrete 403, ter-skip 3. total = 400 data\n",
    "        if i in range(187,190):\n",
    "            continue\n",
    "        else:\n",
    "            filename = f'{name[0]}{i}'\n",
    "            filesname.append(filename)\n",
    "    for i in range(1, 421): #hyperactivity 420, ter-skip 20. total = 400 data\n",
    "        if ((i==77)or(i in range(79,81))or(i==82)or(i==124)or(i==132)or(i==175)or(i in range(207,209)or(i==210)\n",
    "            or(i==228)or(i==246)or(i==304)or(i==308)or(i==336)or(i==344)or(i==366)or(i==374)or(i==406)or(i==416))):\n",
    "            continue\n",
    "        else:\n",
    "            filename = f'{name[1]}{i}'\n",
    "            filesname.append(filename)\n",
    "    for i in range(1, 404): #investment 403, ter-skip 3. total = 400 data\n",
    "        if (i==13)or(i==31)or(i==105):\n",
    "            continue\n",
    "        else:\n",
    "            filename = f'{name[2]}{i}'\n",
    "            filesname.append(filename)\n",
    "    for i in range(1, 402): #neuralnetwork 401, ter-skip 1. total = 400 data\n",
    "        if i==324:\n",
    "            continue\n",
    "        else:\n",
    "            filename = f'{name[3]}{i}'\n",
    "            filesname.append(filename)\n",
    "    for i in range(1, 406): #photosynthesis 405, ter-skip 5. total = 400 data\n",
    "        if (i==139)or(i==157)or(i==168)or(i==205)or(i==384):\n",
    "            continue\n",
    "        else:\n",
    "            filename = f'{name[4]}{i}'\n",
    "            filesname.append(filename)\n",
    "    for i in range(1, 405): #proton 404, ter-skip 4. total = 400 data\n",
    "        if (i==304)or(i in range(320,322)or(i==327)):\n",
    "            continue\n",
    "        else:\n",
    "            filename = f'{name[5]}{i}'\n",
    "            filesname.append(filename)\n",
    "    for i in range(1, 402): #tectonicplates 401, ter-skip 1. total = 400 data\n",
    "        if i==54:\n",
    "            continue\n",
    "        else:\n",
    "            filename = f'{name[6]}{i}'\n",
    "            filesname.append(filename)        \n",
    "    try:\n",
    "        #looping tiap file\n",
    "        index = 0\n",
    "        for files in filesname:\n",
    "            file = open(f\"D:/text clustering/SCOPUS 2.8K/{files}\",\"r\",encoding='utf8') \n",
    "            content = file.read().strip().splitlines() #list berisi tiap baris dalam file\n",
    "            doc = {}\n",
    "            if 0 <= index <= 399:\n",
    "                doc['label'] = 'concrete'\n",
    "            elif 400 <= index <= 799:\n",
    "                doc['label'] = 'hyperactivity'\n",
    "            elif 800 <= index <= 1199:\n",
    "                doc['label'] = 'investment'\n",
    "            elif 1200 <= index <= 1599:\n",
    "                doc['label'] = 'neuralnetwork'\n",
    "            elif 1600 <= index <= 1999:\n",
    "                doc['label'] = 'photosynthesis'\n",
    "            elif 2000 <= index <= 2399:\n",
    "                doc['label'] = 'proton'\n",
    "            elif 2400 <= index <= 2799:\n",
    "                doc['label'] = 'tectonicplates'\n",
    "            else:\n",
    "                print('index out of range')\n",
    "            lines = []\n",
    "            #looping tiap baris dalam 1 file, input ke list\n",
    "            for line in content:\n",
    "                lines.append(line)\n",
    "            #membuat dict tiap file\n",
    "            for l in lines:\n",
    "                doc['title'] = lines[0]\n",
    "                doc['abstract'] = lines[1:] #mengeluarkan value abstract yg berbentuk list\n",
    "                # if len(list_abstract) > 3:  #ada data yg index 0nya berisi \\t, jadi aksesnya langsung dari index 1\n",
    "                #     doc['abstract'] = list_abstract[1] #mengeluarkan value dari list abstract biar jadi string\n",
    "                # else:\n",
    "                #     doc['abstract'] = list_abstract[0]\n",
    "                doc['merged'] = str(doc['title']) + ' ' + str(doc['abstract'])\n",
    "            docs.append(doc)\n",
    "            index += 1\n",
    "\n",
    "        #display data\n",
    "        docs_df = pd.DataFrame.from_dict(docs)\n",
    "        # print(docs_df)\n",
    "\n",
    "        #dataframe to csv\n",
    "        # docs_df.to_csv('D:/text clustering/scopus.csv', index=False, encoding='utf-8')\n",
    "\n",
    "        # check data\n",
    "        # print(f'Jumlah data null : {docs_df.isnull().sum()}\\n')\n",
    "        # print(f'Total data null : {docs_df.isnull().sum().sum()}\\n')\n",
    "        # print(f'Data Duplikat : {docs_df.duplicated()}\\n')\n",
    "    except IOError as e:\n",
    "        print(IOError, e)\n",
    "\n",
    "scopus_data_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Preprocessing:\n",
    "- Punctuation Removal\n",
    "- Case Folding\n",
    "- Tokenization\n",
    "- Stopwords Removal\n",
    "\n",
    "Menghasilkan return berupa list dataset yang telah bebas dari noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = str(text)\n",
    "    punctuation_removal = re.sub(r'\\d', '', text)\n",
    "    punctuation_removal = re.sub(r'[^a-zA-Z\\s]', '', punctuation_removal) #punctuation removal\n",
    "    casefolding = punctuation_removal.casefold() #casefolding\n",
    "    tokenization = word_tokenize(casefolding) #tokenization, outputnya list\n",
    "    stopwords_list = set(stopwords.words('english')) #daftar stopword bhs inggris\n",
    "    filtered = [] #list per kata\n",
    "    for word in tokenization:\n",
    "        if word not in stopwords_list:\n",
    "            filtered.append(word)\n",
    "    return ' '.join(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PUBMED DATASET APPLY PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed = pd.read_csv(\"pubmed.csv\")\n",
    "pubmed.loc[:,'merged'] = pubmed['merged'].apply(preprocessing)\n",
    "pubmed['merged'].to_csv('pubmed_cleaned.csv', index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCOPUS DATASET APPLY PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus = pd.read_csv(\"scopus.csv\")\n",
    "scopus.loc[:,'merged'] = scopus['merged'].apply(preprocessing)\n",
    "scopus['merged'].to_csv('scopus_cleaned.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MERGED PUBMED AND SCOPUS DATASET PREPROCESSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed = pd.read_csv('pubmed_cleaned.csv')\n",
    "scopus = pd.read_csv('scopus_cleaned.csv')\n",
    "\n",
    "dataset = pd.concat([pubmed, scopus], ignore_index=True)\n",
    "dataset.columns = ['preprocessed']\n",
    "# dataset.to_csv('Dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging and Extraction of Nouns Adjective\n",
    "Menghasilkan return berupa dataframe dengan kolom noun, adjective, dan noun adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset.iterrows():\n",
    "    pos_tagging_function = pos_tag(dataset.loc[i[0],'preprocessed'].strip().split(' '))\n",
    "    nouns_list = []\n",
    "    adj_list = []\n",
    "    for tuple_text in pos_tagging_function: #satuan tuple dg index 0 kata, dan index 1 tag\n",
    "        if ((tuple_text[1]=='NN') or (tuple_text[1]=='NNP') or (tuple_text[1]=='NNS')):\n",
    "            nouns_list.append(tuple_text[0])\n",
    "        elif ((tuple_text[1]=='JJ') or (tuple_text[1]=='JJR') or (tuple_text[1]=='JJS')):\n",
    "            adj_list.append(tuple_text[0])\n",
    "        else:\n",
    "            continue\n",
    "        dataset.loc[i[0], 'noun'] = ' '.join(nouns_list)\n",
    "        dataset.loc[i[0], 'adjective'] = ' '.join(adj_list)\n",
    "    dataset.loc[i[0], 'noun adj'] = ' '.join(nouns_list+adj_list)\n",
    "    break\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('Dataset.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Sense Disambiguation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Mencari kata yang ambigu dan tidak ambigu dengan cara mengecek sinonim dari setiap kata:\n",
    "   - Jika suatu kata memiliki jumlah sinonim 1, maka kata tersebut tidak ambigu. Kata tersebut akan masuk ke dalam daftar kata tidak ambigu\n",
    "   - Jika suatu kata memiliki jumlah sinonim > 1, maka kata tersebut ambigu. Kata tersebut akan masuk kedalam daftar kata ambigu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from strsimpy import SIFT4\n",
    "\n",
    "def AmbiguousAndDisambiguous(text):\n",
    "    disambiguous_senses = {}\n",
    "    ambiguous_senses = {}\n",
    "    for word in text:\n",
    "        synset = wordnet.synsets(word)\n",
    "        s = []\n",
    "        for syn in synset:\n",
    "            if ((syn.pos() == 'n') or (syn.pos() == 'a')):\n",
    "                s.append(syn)\n",
    "        if len(s)==1:\n",
    "            disambiguous_senses[word] = s[0]\n",
    "        elif len(s)==2:\n",
    "            ambiguous_senses[word] = s\n",
    "        elif len(s)>=3:\n",
    "            ambiguous_senses[word] = s[0:3]\n",
    "    return (ambiguous_senses, disambiguous_senses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Menentukan sense yang paling tepat dari daftar kata ambigu menggunakan **Wu Palmer Similarity**\n",
    "   \n",
    "$ sim(s_{1}, s_{2}) = \\frac{2d+D}{l_{1} +l_{2}+D} $\n",
    "\n",
    "where:\n",
    "- $l_{1}, l_{2}$ adalah kedalaman sense dari LCS\n",
    "- *d* adalah kedalaman LCS dari root node \n",
    "- $ D = \\frac{1}{sift4(str_{1}, str_{2})}$\n",
    "\n",
    "\n",
    "**Wu Palmer**:\n",
    "\n",
    "**LCS**\n",
    "\n",
    "<img src='lcs.png' width=\"400\" />\n",
    "\n",
    "1. Least Common Subsumer (LCS) merupakan node induk dari kedua sense. Function find_lcs (in=sense1,sense2) -> return lcs+pathnya\n",
    "\t- if lcs = 0, return none\n",
    "\t- if lcs = 1 & path 1, return lcs+pathnya\n",
    "\t- if lcs = 1 & path > 1, return lcs+pathnya terpanjang\n",
    "\t- if lcs > 1 & path 1, return lcs dg path terpanjang dari sense ke root+pathnya\n",
    "\t- if lcs > 1 & path > 1, return lcs dg path terpanjang dari sense ke root+path terpanjangnya\n",
    "2. Hitung depth dari lcs ke root\n",
    "3. Hitung depth dari masing2 s1 dan s2 ke lcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lcs(s1,s2):\n",
    "    lcs = s1.lowest_common_hypernyms(s2)\n",
    "    if len(lcs) == 1:\n",
    "        if len(lcs[0].hypernym_paths()) == 1:\n",
    "            return lcs[0], len(lcs[0].hypernym_paths())\n",
    "        elif len(lcs[0].hypernym_paths()) > 1:\n",
    "            depth = ([len(i) for i in lcs[0].hypernym_paths()])\n",
    "            choosen_path = lcs[0].hypernym_paths()[depth.index(max(depth))]\n",
    "            return lcs[0], len(choosen_path)\n",
    "    elif len(lcs) > 1:\n",
    "        lcs_list = []\n",
    "        for i in lcs:\n",
    "            lcs_list.append([len(j) for j in i.hypernym_paths()])\n",
    "        choosen_lcs = lcs[lcs_list.index(max(lcs_list))]\n",
    "        if len(choosen_lcs.hypernym_paths()) == 1:\n",
    "            return choosen_lcs, len(choosen_lcs.hypernym_paths())\n",
    "        elif len(choosen_lcs.hypernym_paths()) > 1:\n",
    "            depth = ([len(i) for i in choosen_lcs.hypernym_paths()])\n",
    "            choosen_path = choosen_lcs.hypernym_paths()[depth.index(max(depth))]\n",
    "            return choosen_lcs, len(choosen_path)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sense_examples(s1,s2):\n",
    "    s1_ex = s1.examples()\n",
    "    s2_ex = s2.examples()\n",
    "    if not(len(s1_ex)) and not(len(s2_ex)):\n",
    "        return '', ''\n",
    "    elif not(len(s1_ex)) and (len(s2_ex)):\n",
    "        return '', s2_ex[0]\n",
    "    elif (len(s1_ex)) and not(len(s2_ex)):\n",
    "        return s1_ex[0], ''\n",
    "    else:\n",
    "        return s1_ex[0], s2_ex[0]\n",
    "\n",
    "def sense_hyponyms(s1,s2):\n",
    "    s1_hy = s1.hyponyms()\n",
    "    s2_hy = s2.hyponyms()\n",
    "    if not(len(s1_hy)) and not(len(s2_hy)): \n",
    "        return '', ''\n",
    "    elif not(len(s1_hy)) and (len(s2_hy)):\n",
    "        return '', s2_hy[0].definition()\n",
    "    elif (len(s1_hy)) and not(len(s2_hy)):\n",
    "        return s1_hy[0].definition(), ''\n",
    "    else:\n",
    "        return s1_hy[0].definition(), s2_hy[0].definition()\n",
    "    \n",
    "def sense_meronyms(s1,s2):\n",
    "    s1_me = s1.part_meronyms()\n",
    "    s2_me = s2.part_meronyms()\n",
    "    if not(len(s1_me)) and not(len(s2_me)):\n",
    "        return '', ''\n",
    "    elif not(len(s1_me)) and (len(s2_me)):\n",
    "        return '', s2_me[0].definition()\n",
    "    elif (len(s1_me)) and not(len(s2_me)):\n",
    "        return s1_me[0].definition(), ''\n",
    "    else:\n",
    "        return s1_me[0].definition(), s2_me[0].definition()\n",
    "    \n",
    "def sense_holonyms(s1,s2):\n",
    "    s1_ho = s1.part_holonyms()\n",
    "    s2_ho = s2.part_holonyms()\n",
    "    if not(len(s1_ho)) and not(len(s2_ho)):\n",
    "        return '', ''\n",
    "    elif not(len(s1_ho)) and (len(s2_ho)):\n",
    "        return '', s2_ho[0].definition()\n",
    "    elif (len(s1_ho)) and not(len(s2_ho)):\n",
    "        return s1_ho[0].definition(), ''\n",
    "    else:\n",
    "        return s1_ho[0].definition(), s2_ho[0].definition()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_similarity(s1,s2,syn1,syn2):\n",
    "    lcs, d = compute_lcs(s1,s2)\n",
    "    if lcs:\n",
    "        l1 = lcs.shortest_path_distance(s1)\n",
    "        l2 = lcs.shortest_path_distance(s2)\n",
    "        s1_ex, s2_ex = sense_examples(s1,s2)\n",
    "        s1_hy, s2_hy = sense_hyponyms(s1,s2)\n",
    "        s1_me, s2_me = sense_meronyms(s1,s2)\n",
    "        s1_ho, s2_ho = sense_holonyms(s1,s2)\n",
    "        D = SIFT4().distance(s1.definition()+' '+s1_ex+' '+syn1+' '+lcs.definition()+' '+s1_hy+' '+s1_me+' '+s1_ho,s2.definition()+' '+s2_ex+' '+syn2+' '+lcs.definition()+' '+s2_hy+' '+s2_me+' '+s2_ho)\n",
    "        sim = (2*d+(D**-1))/(l1+l2+(D**-1))\n",
    "        return sim\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def adj_similarity(s1,s2,syn1,syn2):\n",
    "    d = len(s1.hypernym_paths()) + len(s2.hypernym_paths())\n",
    "    l1 = s1.shortest_path_distance(s1)\n",
    "    l2 = s2.shortest_path_distance(s2)\n",
    "    s1_ex, s2_ex = sense_examples(s1,s2)\n",
    "    D = SIFT4().distance(s1.definition()+' '+s1_ex+' '+syn1,s2.definition()+' '+s2_ex+' '+syn2)\n",
    "    sim = (2*d+(D**-1))/(l1+l2+(D**-1))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wsd(amb,disamb):\n",
    "    disambiguated = []\n",
    "    a = [s1 for s in amb.values() for s1 in s]\n",
    "    d = [s2 for s2 in disamb.values()]\n",
    "    comparison = a + d\n",
    "    choosen = {}\n",
    "    for s1_keys in amb: #looping each word's synset\n",
    "        for s1 in amb[s1_keys]:\n",
    "            sense_dict = {}\n",
    "            for s2_keys in disamb:\n",
    "                for s2 in comparison:\n",
    "                    syn_rel1 = wordnet.synsets(s1_keys)[0].lemma_names()[0]\n",
    "                    syn_rel2 = wordnet.synsets(s2_keys)[0].lemma_names()[0]\n",
    "                    if ((s1.pos() == 'n') and (s2.pos() == 'n')) and (s1 != s2):\n",
    "                        sim = noun_similarity(s1,s2,syn_rel1,syn_rel2)\n",
    "                        if sim == 0:\n",
    "                            sense_dict[s1] = 0\n",
    "                        sense_dict[s1] = sim\n",
    "                    elif ((s1.pos() == 'a') and (s2.pos() == 'a') and (s1 != s2)):\n",
    "                        sim = adj_similarity(s1,s2,syn_rel1,syn_rel2)\n",
    "                        sense_dict[s1] = sim\n",
    "                    else:\n",
    "                        continue\n",
    "        value = [i for i in sense_dict.values()]\n",
    "        best = list(filter(lambda x: sense_dict[x] == max(value), sense_dict))\n",
    "        if len(best) >= 1:\n",
    "            choosen[s1_keys] = best[0]\n",
    "        else:\n",
    "            continue\n",
    "    disambiguated.append(choosen)\n",
    "    return disambiguated[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = pd.read_csv('Dataset.csv')\n",
    "texts = data['noun adj'].to_numpy()\n",
    "senses_docs = []\n",
    "for text in tqdm(texts):\n",
    "    sense_doc = {}\n",
    "    noun_adj_list = text.strip().split(' ')\n",
    "    amb, disamb = AmbiguousAndDisambiguous(noun_adj_list)\n",
    "    disambiguated = wsd(amb,disamb)\n",
    "    print(disambiguated)\n",
    "    # break\n",
    "    # word_sense = disamb.copy()\n",
    "    # word_sense.update(disambiguated)\n",
    "    # sense_doc['senses'] = word_sense\n",
    "    # senses_docs.append(sense_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sense = pd.DataFrame.from_dict(senses_docs)\n",
    "df_sense.to_csv('features.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical (TF-IDF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input = set of disambiguated sense\n",
    "<br>output = map of sense as a keys and tf-idf as a values\n",
    "\n",
    "$ TF-IDF(s_{i}) = freq(s_{i}) * (log(\\frac{|D|+1}{doc\\_count(s_{i})+1})+1) $\n",
    "\n",
    "where\n",
    "\n",
    "- $freq(s_{i})$ = total sense of a document\n",
    "- |*D*| = total number of document in corpus\n",
    "- $doc\\_count(s_{i})$ = total number of document that contain term corresponding to sense $(s_{i})$\n",
    "\n",
    "\n",
    "1. Compute total sense\n",
    "2. Compute total document\n",
    "3. Compute total document that contain the term corresponding to sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic (Lexical Chain) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of All Words (BOAW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Dataset.csv')\n",
    "vectorizer = CountVectorizer()\n",
    "for i in data.iterrows():\n",
    "    X = vectorizer.fit_transform(data.loc[i[0],'preprocessed'].strip().split(' '))\n",
    "    vectorizer.get_feature_names_out()\n",
    "    print(X.toarray())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Nouns (BON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "for i in dataset.iterrows():\n",
    "    X = vectorizer.fit_transform(dataset.loc[i[0],'noun'].strip().split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Nouns and Adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "for i in dataset.iterrows():\n",
    "    X = vectorizer.fit_transform(dataset.loc[i[0],'noun adj'].strip().split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spherical K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted Mutual Information (AMI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab696253bf1cbd9262a120019f89a6af5e719602af9132e0a92b18d085125844"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
